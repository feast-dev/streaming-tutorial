{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building streaming features with Feast, Spark, and Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "This notebook explores how data scientists and engineers can build streaming features in Feast.\n",
    "It builds off of the first [module](https://github.com/feast-dev/feast-workshop/tree/main/module_1) from the Feast workshop.\n",
    "The workshop module is not a prerequisite, but is recommended if you are not already familiar with the Push API in Feast.\n",
    "\n",
    "Streaming features are a powerful tool to decrease training-serving skew.\n",
    "If built correctly, models in production can use extremely fresh features, often yielding dramatic performance improvements.\n",
    "However, there are many challenges around building streaming features.\n",
    "For example, how does a data scientist define streaming features?\n",
    "How does the data scientist connect the stream data source with the appropriate batch data source?\n",
    "What if there are transformations involved?\n",
    "Who is responsible for keeping the streaming features alive in production?\n",
    "How do they ensure that the streaming features in production are consistent with the features available for training in an offline setting?\n",
    "Feast can help solve these problems.\n",
    "\n",
    "All the necessary resources to run this notebook are in `kafka_spark_demo`.\n",
    "These include a sample feature repo and Dockerfiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "A typical pattern that we see is that data scientists use Feast to define, use, and share features, which greatly improves their productivity, while ML engineers use Feast to ensure that all the features are available for both training and serving.\n",
    "This notebook is organized as follows.\n",
    "* Sections 2 and 3 do some basic setup.\n",
    "* Sections 4 and 5 show how a data scientist can define and use stream features in Feast.\n",
    "* Section 6 then shows how an ML engineer can use the Push API to ensure that those streaming features are available in production.\n",
    "\n",
    "All the capabilities discussed here are currently available in the latest version of Feast (0.22.1).\n",
    "The only capability that requires custom logic is ingesting features from a stream data source (in section 6), which would typically be handled by an ML platform team.\n",
    "We have written sample code to use Spark Structured Streaming to ingest features from a Kafka data source, and the approach we used can be modified to work for other stream processing engines and stream data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up feature store, Kafka, and Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply feature repository\n",
    "We first run `feast apply` to register our data sources and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:100: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\r\n",
      "  warnings.warn(\r\n",
      "Created entity \u001b[1m\u001b[32mdriver\u001b[0m\r\n",
      "Created entity \u001b[1m\u001b[32mcustomer\u001b[0m\r\n",
      "Created feature view \u001b[1m\u001b[32mcustomer_stats\u001b[0m\r\n",
      "Created stream feature view \u001b[1m\u001b[32mdriver_hourly_stats_stream\u001b[0m\r\n",
      "\r\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mcustomer_stats\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!feast apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spin up Kafka + Redis\n",
    "\n",
    "We then use Docker Compose to spin up Kafka and Redis. Kafka simulates the streaming infrastructure that provides data for the `driver_hourly_stats_stream` feature view, and Redis is used for the online store.\n",
    "\n",
    "We can spin these services up by running `docker-compose up` from the `kafka_spark_demo` directory. This leverages a script (in `kafka_demo/`) that creates a topic, reads from `feature_repo/data/driver_stats_stream.parquet`, generates newer timestamps, and emits them to the topic. It also deploys an instance of Redis.\n",
    "\n",
    "```\n",
    "$ docker-compose up\n",
    "Creating network \"kafka_spark_demo_default\" with the default driver\n",
    "Creating redis     ... done\n",
    "Creating zookeeper ... done\n",
    "Creating broker    ... done\n",
    "Creating kafka_events ... done\n",
    "Attaching to zookeeper, redis, broker, kafka_events\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore existing feature views\n",
    "\n",
    "Let's assume the role of a data scientist who wants to train a model to determine which customers and drivers should be matched together. We start by exploring the existing feature views, as another data scientist might have already defined a useful feature view.\n",
    "\n",
    "If we inspect `features.py`, we'll see that a `customer_stats_view` feature view already exists. Let's inspect the feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch training data from offline store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id                  event_timestamp  current_balance  \\\n",
      "0         5001        2021-04-12 10:59:42+00:00         0.174109   \n",
      "1         5003        2021-04-12 16:40:26+00:00         0.735872   \n",
      "2         5004        2021-04-12 15:01:12+00:00         0.885541   \n",
      "3         5002        2021-04-12 08:12:10+00:00         0.922777   \n",
      "4         5001 2022-06-27 15:19:26.431431+00:00         0.544859   \n",
      "\n",
      "   avg_passenger_count  lifetime_trip_count  \n",
      "0             0.384933                   14  \n",
      "1             0.542926                  616  \n",
      "2             0.774241                  129  \n",
      "3             0.167704                  844  \n",
      "4             0.087846                  240  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:100: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"customer_id\": [5001, 5002, 5003, 5004, 5001],\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2021, 4, 12, 10, 59, 42),\n",
    "            datetime(2021, 4, 12, 8, 12, 10),\n",
    "            datetime(2021, 4, 12, 16, 40, 26),\n",
    "            datetime(2021, 4, 12, 15, 1, 12),\n",
    "            datetime.now()\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"customer_stats:current_balance\",\n",
    "        \"customer_stats:avg_passenger_count\",\n",
    "        \"customer_stats:lifetime_trip_count\",\n",
    "    ],\n",
    ").to_df()\n",
    "print(training_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create stream feature view\n",
    "\n",
    "We're satisfied with the `customer_stats` feature view, as it will provide all the necessary features for a customer. But we still needs features for drivers. Moreover, the streaming team has told us that there is actually a Kafka stream that can provide extremely fresh features for drivers. In order to take advantage of those fresh features, we just need to define a `StreamFeatureView` instead of a standard `FeatureView`, and pass that Kafka source to the `StreamFeatureView`. For convenience, the feature repo already contains the `KafkaSource` and `StreamFeatureView`.\n",
    "\n",
    "Let's take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from datetime import timedelta\r\n",
      "\r\n",
      "from feast import (\r\n",
      "    FileSource,\r\n",
      "    KafkaSource,\r\n",
      ")\r\n",
      "from feast.data_format import JsonFormat\r\n",
      "\r\n",
      "# Feast also supports pulling data from data warehouses like BigQuery, Snowflake, Redshift and data lakes (e.g. via Redshift Spectrum, Trino, Spark)\r\n",
      "customer_stats_batch_source = FileSource(\r\n",
      "    name=\"customer_stats_source\",\r\n",
      "    path=\"data/customer_stats.parquet\",\r\n",
      "    timestamp_field=\"event_timestamp\",\r\n",
      "    created_timestamp_column=\"created\",\r\n",
      "    description=\"A table describing the stats of a customer based on daily logs\",\r\n",
      "    owner=\"test1@gmail.com\",\r\n",
      ")\r\n",
      "\r\n",
      "driver_stats_batch_source = FileSource(\r\n",
      "    name=\"driver_stats_source\",\r\n",
      "    path=\"data/driver_stats.parquet\",\r\n",
      "    timestamp_field=\"event_timestamp\",\r\n",
      "    created_timestamp_column=\"created\",\r\n",
      "    description=\"A table describing the stats of a driver based on hourly logs\",\r\n",
      "    owner=\"test2@gmail.com\",\r\n",
      ")\r\n",
      "\r\n",
      "driver_stats_stream_source = KafkaSource(\r\n",
      "    name=\"driver_stats_stream\",\r\n",
      "    kafka_bootstrap_servers=\"localhost:9092\",\r\n",
      "    topic=\"drivers\",\r\n",
      "    timestamp_field=\"event_timestamp\",\r\n",
      "    batch_source=driver_stats_batch_source,\r\n",
      "    message_format=JsonFormat(\r\n",
      "        schema_json=\"driver_id integer, event_timestamp timestamp, conv_rate double, acc_rate double, created timestamp\"\r\n",
      "    ),\r\n",
      "    watermark_delay_threshold=timedelta(minutes=5),\r\n",
      "    description=\"The Kafka stream containing the driver stats\",\r\n",
      "    owner=\"test3@gmail.com\",\r\n",
      ")\r\n"
     ]
    }
   ],
   "source": [
    "!cat data_sources.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a data scientist, we don't need to know too much about this Kafka stream. All we need are the bootstrap servers, the topic, and the schema to define the `KafkaSource`. We can rely on our ML platform team to ensure that the stream is up and running in production (which will be covered by a later section in this notebook). Note that we also specify a `batch_source` in the definition of this `KafkaSource`, which is the `driver_stats_batch_source` object defined above. This batch source is where our training data lives. When we want historical data to train our model, we will retrieve it from the batch source. Moreover, our ML platform team will also ensure that any streaming data that is available in production will eventually make its way into the batch source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a closer look at the stream feature view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from datetime import timedelta\r\n",
      "from pyspark.sql import DataFrame\r\n",
      "\r\n",
      "from feast import (\r\n",
      "    FeatureView,\r\n",
      "    Field,\r\n",
      ")\r\n",
      "from feast.stream_feature_view import stream_feature_view\r\n",
      "from feast.types import Float32, Int32\r\n",
      "\r\n",
      "from data_sources import *\r\n",
      "from entities import *\r\n",
      "\r\n",
      "customer_stats_view = FeatureView(\r\n",
      "    name=\"customer_stats\",\r\n",
      "    description=\"customer features\",\r\n",
      "    entities=[customer],\r\n",
      "    ttl=timedelta(seconds=8640000000),\r\n",
      "    schema=[\r\n",
      "        Field(name=\"current_balance\", dtype=Float32),\r\n",
      "        Field(name=\"avg_passenger_count\", dtype=Float32),\r\n",
      "        Field(name=\"lifetime_trip_count\", dtype=Int32),\r\n",
      "    ],\r\n",
      "    online=True,\r\n",
      "    source=customer_stats_batch_source,\r\n",
      "    tags={\"production\": \"True\"},\r\n",
      "    owner=\"test1@gmail.com\",\r\n",
      ")\r\n",
      "\r\n",
      "@stream_feature_view(\r\n",
      "    entities=[driver],\r\n",
      "    ttl=timedelta(seconds=8640000000),\r\n",
      "    mode=\"spark\",\r\n",
      "    schema=[\r\n",
      "        Field(name=\"conv_percentage\", dtype=Float32),\r\n",
      "        Field(name=\"acc_percentage\", dtype=Float32),\r\n",
      "    ],\r\n",
      "    timestamp_field=\"event_timestamp\",\r\n",
      "    online=True,\r\n",
      "    source=driver_stats_stream_source,\r\n",
      "    tags={},\r\n",
      ")\r\n",
      "def driver_hourly_stats_stream(df: DataFrame):\r\n",
      "    from pyspark.sql.functions import col\r\n",
      "\r\n",
      "    return (\r\n",
      "        df.withColumn(\"conv_percentage\", col(\"conv_rate\") * 100.0)\r\n",
      "        .withColumn(\"acc_percentage\", col(\"acc_rate\") * 100.0)\r\n",
      "        .drop(\"conv_rate\", \"acc_rate\")\r\n",
      "    )\r\n"
     ]
    }
   ],
   "source": [
    "!cat features.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stream feature view requires mostly the same parameters as a normal feature view. One new capability with stream feature views is the ability to define an associated transformation. In this case, we can see that the transformation is a pyspark udf that will transform the rates into percentages by multiplying them by 100. This might be used in a situation where the features in the stream are in decimal format, e.g. `0.5211`, but we want to use percentages, e.g. `52.11`. Since we added a transformation, we also specified `mode=spark`, which indicates that we will use pyspark to process the stream.\n",
    "\n",
    "Let's also inspect the historical features for the stream feature view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   driver_id                  event_timestamp  conv_percentage  acc_percentage\n",
      "0       1001        2021-04-12 10:59:42+00:00        52.114902       75.165855\n",
      "1       1003        2021-04-12 16:40:26+00:00        18.885477       34.473606\n",
      "2       1004        2021-04-12 15:01:12+00:00        29.649216       93.530525\n",
      "3       1002        2021-04-12 08:12:10+00:00         8.901370       21.263689\n",
      "4       1001 2022-06-27 15:19:27.924174+00:00        40.458847       40.757076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:100: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"driver_id\": [1001, 1002, 1003, 1004, 1001],\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2021, 4, 12, 10, 59, 42),\n",
    "            datetime(2021, 4, 12, 8, 12, 10),\n",
    "            datetime(2021, 4, 12, 16, 40, 26),\n",
    "            datetime(2021, 4, 12, 15, 1, 12),\n",
    "            datetime.now()\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"driver_hourly_stats_stream:conv_percentage\",\n",
    "        \"driver_hourly_stats_stream:acc_percentage\",\n",
    "    ],\n",
    ").to_df()\n",
    "print(training_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what we need to train our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Productionize stream feature view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can materialize the features from our batch source into the online store to use them in production. The features will not be very fresh as we are not taking advantage of the Kafka stream, but we will handle this in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:100: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n",
      "Materializing \u001b[1m\u001b[32m2\u001b[0m feature views to \u001b[1m\u001b[32m2022-06-26 17:00:00-07:00\u001b[0m into the \u001b[1m\u001b[32mredis\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mcustomer_stats\u001b[0m from \u001b[1m\u001b[32m1748-09-11 22:19:35-07:52:58\u001b[0m to \u001b[1m\u001b[32m2022-06-26 17:00:00-07:00\u001b[0m:\n",
      "100%|████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 213.68it/s]\n",
      "\u001b[1m\u001b[32mdriver_hourly_stats_stream\u001b[0m from \u001b[1m\u001b[32m1748-09-11 22:19:36-07:52:58\u001b[0m to \u001b[1m\u001b[32m2022-06-26 17:00:00-07:00\u001b[0m:\n",
      "100%|████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 670.60it/s]\n",
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:100: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!feast materialize-incremental $(date +%Y-%m-%d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that the features have been materialized correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_percentage  :  [40.757076263427734]\n",
      "acc_percentage__ts  :  [1647266400]\n",
      "conv_percentage  :  [40.45884704589844]\n",
      "conv_percentage__ts  :  [1647266400]\n",
      "driver_id  :  [1001]\n",
      "driver_id__ts  :  [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:100: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats_stream:conv_percentage\",\n",
    "        \"driver_hourly_stats_stream:acc_percentage\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict(include_event_timestamps=True)\n",
    "\n",
    "def print_online_features(features):\n",
    "    for key, value in sorted(features.items()):\n",
    "        print(key, \" : \", value)\n",
    "\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ingesting and transforming data from a Kafka topic\n",
    "\n",
    "Now let's switch to the perspective of an ML engineer. We see that a data scientist has just registered a stream feature view, and we are now responsible for ensuring that fresh features from the stream are available in production.\n",
    "\n",
    "We will use Spark Structured Streaming to ingest from a Kafka topic, transform the data, and then push the data to the online store. We start by setting up a Spark session, as well as importing several other modules we'll use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/felixwang/feast/env/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/felixwang/feast/env/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/felixwang/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/felixwang/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-406f197c-467a-49a5-9f61-1bc4c65719db;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in local-m2-cache\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 337ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from local-m2-cache in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-406f197c-467a-49a5-9f61-1bc4c65719db\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/9ms)\n",
      "22/06/27 15:19:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# See https://spark.apache.org/docs/3.1.2/structured-streaming-kafka-integration.html#deploying for notes on why we need this environment variable.\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell\"\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"feast-spark\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feast currently does not support launching and orchestrating Spark jobs to read from Kafka; any Feast user that wishes to handle streaming data must write custom logic to manage their own streaming infrastructure.\n",
    "As an example of what that might look like for Spark and Kafka, please see the sample code (~200 lines) [here](https://github.com/feast-dev/feast/tree/master/sdk/python/feast/infra/contrib).\n",
    "\n",
    "**The sample code contains all the necessary custom logic necessary to ingest and transform streaming data for Feast.**\n",
    "\n",
    "It requires only three things.\n",
    "First, the feature store object, which will push the data into the online store.\n",
    "Second, the stream feature view whose stream data source will be ingested.\n",
    "And third, a config object for the stream processing engine of choice.\n",
    "An optional fourth parameter is a preprocessing function, which allows you to execute custom logic after the transformation has been executed, but before the data has been written into the online store.\n",
    "Since we are working in a notebook, we will simply use the preprocessing function to print.\n",
    "\n",
    "Now let's set up each of these items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast.infra.contrib.stream_processor import ProcessorConfig\n",
    "from feast.infra.contrib.spark_kafka_processor import SparkProcessorConfig\n",
    "from feast.infra.contrib.stream_processor import get_stream_processor_object\n",
    "\n",
    "def preprocess_fn(rows: pd.DataFrame):\n",
    "    print(f\"df columns: {rows.columns}\")\n",
    "    print(f\"df size: {rows.size}\")\n",
    "    print(f\"df preview:\\n{rows.head()}\")\n",
    "    return rows\n",
    "\n",
    "ingestion_config = SparkProcessorConfig(mode=\"spark\", source=\"kafka\", spark_session=spark, processing_time=\"30 seconds\", query_timeout=15)\n",
    "sfv = store.get_stream_feature_view(\"driver_hourly_stats_stream\")\n",
    "\n",
    "processor = get_stream_processor_object(\n",
    "    config=ingestion_config,\n",
    "    fs=store,\n",
    "    sfv=sfv,\n",
    "    preprocess_fn=preprocess_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready!\n",
    "\n",
    "We have already materialized features from the offline store and retrieved them, so we expect that after our ingestion job, any features we retrieve will be fresher and have more recent timestamps.\n",
    "\n",
    "Now we launch the ingestion job.\n",
    "Let's let it run for a little bit to ensure that the job is indeed pushing data to the online store.\n",
    "Note that in the configuration above we set the processing time to be 30 seconds, so it might take a little while before we see a non-empty dataframe being ingested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/27 15:20:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "/Users/felixwang/feast/env/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df columns: Index(['event_timestamp', 'created', 'conv_percentage', 'acc_percentage'], dtype='object')\n",
      "df size: 0\n",
      "df preview:\n",
      "Empty DataFrame\n",
      "Columns: [event_timestamp, created, conv_percentage, acc_percentage]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixwang/feast/env/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:100: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df columns: Index(['event_timestamp', 'created', 'conv_percentage', 'acc_percentage'], dtype='object')\n",
      "df size: 20\n",
      "df preview:\n",
      "              event_timestamp                          created  \\\n",
      "driver_id                                                        \n",
      "1001      2024-02-25 19:00:00 2022-06-27 22:20:31.158722+00:00   \n",
      "1002      2024-02-25 19:00:00 2022-06-27 22:20:31.158722+00:00   \n",
      "1003      2024-02-25 18:00:00 2022-06-27 22:20:31.158722+00:00   \n",
      "1004      2024-02-25 19:00:00 2022-06-27 22:20:31.158722+00:00   \n",
      "1005      2024-02-25 19:00:00 2022-06-27 22:20:31.158722+00:00   \n",
      "\n",
      "           conv_percentage  acc_percentage  \n",
      "driver_id                                   \n",
      "1001             67.108572       38.001445  \n",
      "1002             51.633394       15.392214  \n",
      "1003             70.622385       46.187967  \n",
      "1004             90.648526        3.402347  \n",
      "1005             15.650299       96.641272  \n"
     ]
    }
   ],
   "source": [
    "query = processor.ingest_stream_feature_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we retrieve features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_percentage  :  [38.00144577026367]\n",
      "acc_percentage__ts  :  [1708887600]\n",
      "conv_percentage  :  [67.10857391357422]\n",
      "conv_percentage__ts  :  [1708887600]\n",
      "driver_id  :  [1001]\n",
      "driver_id__ts  :  [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:100: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats_stream:conv_percentage\",\n",
    "        \"driver_hourly_stats_stream:acc_percentage\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict(include_event_timestamps=True)\n",
    "\n",
    "def print_online_features(features):\n",
    "    for key, value in sorted(features.items()):\n",
    "        print(key, \" : \", value)\n",
    "\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new features are more fresh! We can see that the feature values are different and the timestamps are more recent.\n",
    "\n",
    "Let's stop the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Finally, let's clean up the checkpoint directories from Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "dir_path = '/tmp/checkpoint'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(dir_path)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s : %s\" % (dir_path, e.strerror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Closing thoughts and future work\n",
    "\n",
    "Although this workflow will allow data scientists to take advantage of streaming features, there are still many ways it can be improved. Here are a few things the Feast team is planning to work on in the near future:\n",
    "* A unified Push API. This will enable ML engineers to push streaming data to the offline store as well as the online store. This capability is already shipped, and will be integrated into this tutorial soon!\n",
    "* A higher-level DSL for aggregations. This will enable data scientists to express common aggregate features (e.g. mean over the last 24 hours) with a DSL, instead of having to write a complex pyspark UDF on their own.\n",
    "* The ability to run the stream transformation in batch mode. Stream data sources currently require a batch data source, and assume that the batch data source contains the transformed feature values. However, many teams choose to persist untransformed feature values to batch sources. If stream transformations can also be run in batch mode, teams can use their existing batch data sources."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d634b9af180bcb32a446a43848522733ff8f5bbf0cc46dba1a83bede04bf237"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
